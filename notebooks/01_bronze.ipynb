{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409d3d-41a6-47ec-9474-b392da225d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from lib.spark_session import get_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4902894e-cdeb-4eed-88f4-a0f81e6c440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: 8\n",
      "25/07/31 11:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/31 11:46:19 WARN DependencyUtils: Local jar /opt/spark/jars/openlineage-spark_2.12-1.33.0.jar does not exist, skipping.\n",
      "25/07/31 11:46:19 INFO SparkContext: Running Spark version 3.5.6\n",
      "25/07/31 11:46:19 INFO SparkContext: OS info Linux, 6.14.0-24-generic, amd64\n",
      "25/07/31 11:46:19 INFO SparkContext: Java version 17.0.15\n",
      "25/07/31 11:46:19 INFO ResourceUtils: ==============================================================\n",
      "25/07/31 11:46:19 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/31 11:46:19 INFO ResourceUtils: ==============================================================\n",
      "25/07/31 11:46:19 INFO SparkContext: Submitted application: Bronze\n",
      "25/07/31 11:46:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/31 11:46:19 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "25/07/31 11:46:19 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/31 11:46:19 INFO SecurityManager: Changing view acls to: root\n",
      "25/07/31 11:46:19 INFO SecurityManager: Changing modify acls to: root\n",
      "25/07/31 11:46:19 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/31 11:46:19 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/31 11:46:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/07/31 11:46:19 INFO Utils: Successfully started service 'sparkDriver' on port 41411.\n",
      "25/07/31 11:46:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/31 11:46:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/31 11:46:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/31 11:46:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/31 11:46:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/31 11:46:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2dee853f-ad95-4fdf-ba7f-e9fafd8e4743\n",
      "25/07/31 11:46:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/07/31 11:46:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/31 11:46:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/31 11:46:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/07/31 11:46:20 INFO SparkContext: Added JAR file:///opt/spark/jars/postgresql-42.6.0.jar at spark://d7020f36ea8c:41411/jars/postgresql-42.6.0.jar with timestamp 1753962379649\n",
      "25/07/31 11:46:20 INFO SparkContext: Added JAR file:///opt/spark/jars/hadoop-aws-3.3.4.jar at spark://d7020f36ea8c:41411/jars/hadoop-aws-3.3.4.jar with timestamp 1753962379649\n",
      "25/07/31 11:46:20 INFO SparkContext: Added JAR file:///opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar at spark://d7020f36ea8c:41411/jars/aws-java-sdk-bundle-1.12.262.jar with timestamp 1753962379649\n",
      "25/07/31 11:46:20 INFO SparkContext: Added JAR file:///opt/spark/jars/delta-spark_2.12-3.2.0.jar at spark://d7020f36ea8c:41411/jars/delta-spark_2.12-3.2.0.jar with timestamp 1753962379649\n",
      "25/07/31 11:46:20 INFO SparkContext: Added JAR file:///opt/spark/jars/delta-storage-3.2.0.jar at spark://d7020f36ea8c:41411/jars/delta-storage-3.2.0.jar with timestamp 1753962379649\n",
      "25/07/31 11:46:20 ERROR SparkContext: Failed to add file:/opt/spark/jars/openlineage-spark_2.12-1.33.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/jars/openlineage-spark_2.12-1.33.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/31 11:46:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "25/07/31 11:46:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.6:7077 after 16 ms (0 ms spent in bootstraps)\n",
      "25/07/31 11:46:20 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250731114620-0000\n",
      "25/07/31 11:46:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45503.\n",
      "25/07/31 11:46:20 INFO NettyBlockTransferService: Server created on d7020f36ea8c:45503\n",
      "25/07/31 11:46:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/31 11:46:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d7020f36ea8c, 45503, None)\n",
      "25/07/31 11:46:20 INFO BlockManagerMasterEndpoint: Registering block manager d7020f36ea8c:45503 with 434.4 MiB RAM, BlockManagerId(driver, d7020f36ea8c, 45503, None)\n",
      "25/07/31 11:46:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d7020f36ea8c, 45503, None)\n",
      "25/07/31 11:46:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d7020f36ea8c, 45503, None)\n",
      "25/07/31 11:46:20 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/events/app-20250731114620-0000.inprogress\n",
      "25/07/31 11:46:20 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "25/07/31 11:46:20 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "25/07/31 11:46:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250731114620-0000/0 on worker-20250731114353-172.21.0.4-45437 (172.21.0.4:45437) with 2 core(s)\n",
      "25/07/31 11:46:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250731114620-0000/0 on hostPort 172.21.0.4:45437 with 2 core(s), 1024.0 MiB RAM\n",
      "25/07/31 11:46:20 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/07/31 11:46:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250731114620-0000/0 is now RUNNING\n",
      "25/07/31 11:46:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.4:50122) with ID 0,  ResourceProfileId 0\n",
      "25/07/31 11:46:22 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "25/07/31 11:46:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.4:37847 with 434.4 MiB RAM, BlockManagerId(0, 172.21.0.4, 37847, None)\n"
     ]
    }
   ],
   "source": [
    "# Generate SparkSession\n",
    "spark: SparkSession = get_spark_session(\"Bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5901d52-b711-4c91-9713-afce7f56f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 11:46:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/31 11:46:34 INFO SharedState: Warehouse path is 'file:/home/jupyter/notebooks/spark-warehouse'.\n",
      "25/07/31 11:46:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/07/31 11:46:37 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/07/31 11:46:37 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
      "25/07/31 11:46:37 INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore)` is used for scheme `s3a`\n",
      "25/07/31 11:46:37 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
      "25/07/31 11:46:37 INFO InitialSnapshot: [tableId=9f1f7bb8-3954-42e4-aae5-93dbf7987391] Created snapshot InitialSnapshot(path=s3a://bronze/employe/_delta_log, version=-1, metadata=Metadata(7489bc48-6eaf-49b4-8003-402aaddc6c5f,null,null,Format(parquet,Map()),null,List(),Map(),Some(1753962397640)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,-1), checksumOpt=None)\n",
      "25/07/31 11:46:37 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
      "25/07/31 11:46:37 INFO InitialSnapshot: [tableId=7489bc48-6eaf-49b4-8003-402aaddc6c5f] Created snapshot InitialSnapshot(path=s3a://bronze/employe/_delta_log, version=-1, metadata=Metadata(c8e4811a-d2b0-4e33-bf06-19269b109564,null,null,Format(parquet,Map()),null,List(),Map(),Some(1753962397683)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,-1), checksumOpt=None)\n",
      "25/07/31 11:46:37 INFO OptimisticTransaction: [tableId=c8e4811a,txnId=bf1278f0] Updated metadata from - to Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1753962397724))\n",
      "25/07/31 11:46:37 INFO OptimisticTransaction: [tableId=c8e4811a,txnId=bf1278f0] Attempting to commit version 0 with 3 actions with Serializable isolation level\n",
      "25/07/31 11:46:38 INFO DeltaLog: Creating a new snapshot v0 for commit version 0\n",
      "25/07/31 11:46:38 INFO DeltaLog: Loading version 0.\n",
      "25/07/31 11:46:38 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 952)\n",
      "25/07/31 11:46:38 INFO DataSourceStrategy: Pruning directories with: \n",
      "25/07/31 11:46:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/31 11:46:38 INFO FileSourceStrategy: Post-Scan Filters: ((isnotnull(protocol#15.minReaderVersion) OR isnotnull(metaData#14.id)) OR (isnotnull(commitInfo#16.inCommitTimestamp) AND (version#17L = 0)))\n",
      "25/07/31 11:46:38 INFO CodeGenerator: Code generated in 198.169373 ms\n",
      "25/07/31 11:46:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 205.2 KiB, free 434.2 MiB)\n",
      "25/07/31 11:46:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)\n",
      "25/07/31 11:46:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d7020f36ea8c:45503 (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:39 INFO SparkContext: Created broadcast 0 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/31 11:46:39 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Got job 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
      "25/07/31 11:46:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 55.3 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d7020f36ea8c:45503 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 11:46:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/31 11:46:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/31 11:46:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.0.4, executor 0, partition 0, PROCESS_LOCAL, 10113 bytes) \n",
      "25/07/31 11:46:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.4:37847 (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.4:37847 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1798 ms on 172.21.0.4 (executor 0) (1/1)\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/31 11:46:41 INFO DAGScheduler: ResultStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.830 s\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Job 0 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.851995 s\n",
      "25/07/31 11:46:41 INFO CodeGenerator: Code generated in 47.98912 ms             \n",
      "25/07/31 11:46:41 INFO Snapshot: [tableId=c8e4811a-d2b0-4e33-bf06-19269b109564] Created snapshot Snapshot(path=s3a://bronze/employe/_delta_log, version=0, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,0,List(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962398000), checksumOpt=None)\n",
      "25/07/31 11:46:41 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://bronze/employe/_delta_log, version=0, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,0,List(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962398000), checksumOpt=None)\n",
      "25/07/31 11:46:41 INFO OptimisticTransaction: [tableId=c8e4811a,txnId=bf1278f0] Committed delta #0 to s3a://bronze/employe/_delta_log\n",
      "25/07/31 11:46:41 INFO CreateDeltaTableCommand: Table is path-based table: false. Update catalog with mode: Create\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on d7020f36ea8c:45503 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.0.4:37847 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO DeltaLog: Loading version 0.\n",
      "25/07/31 11:46:41 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 952)\n",
      "25/07/31 11:46:41 INFO DataSourceStrategy: Pruning directories with: \n",
      "25/07/31 11:46:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/31 11:46:41 INFO FileSourceStrategy: Post-Scan Filters: ((isnotnull(protocol#49.minReaderVersion) OR isnotnull(metaData#48.id)) OR (isnotnull(commitInfo#50.inCommitTimestamp) AND (version#51L = 0)))\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 205.2 KiB, free 434.0 MiB)\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d7020f36ea8c:45503 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:41 INFO SparkContext: Created broadcast 2 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d7020f36ea8c:45503 in memory (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.0.4:37847 in memory (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Got job 1 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 55.3 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on d7020f36ea8c:45503 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/07/31 11:46:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.0.4, executor 0, partition 0, PROCESS_LOCAL, 10113 bytes) \n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.4:37847 (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.4:37847 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 67 ms on 172.21.0.4 (executor 0) (1/1)\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/07/31 11:46:41 INFO DAGScheduler: ResultStage 1 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.073 s\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Job 1 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.076454 s\n",
      "25/07/31 11:46:41 INFO Snapshot: [tableId=5eafe551-376f-4460-b535-eab981c95ca1] Created snapshot Snapshot(path=s3a://bronze/employe/_delta_log, version=0, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398006; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962398006), checksumOpt=None)\n",
      "25/07/31 11:46:41 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://bronze/employe/_delta_log, version=0, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398006; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962398006), checksumOpt=None)\n",
      "25/07/31 11:46:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/07/31 11:46:41 INFO CodeGenerator: Code generated in 5.159964 ms\n",
      "25/07/31 11:46:41 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on d7020f36ea8c:45503 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 336.3 KiB, free 433.8 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.21.0.4:37847 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 122.8 KiB, free 433.7 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on d7020f36ea8c:45503 (size: 122.8 KiB, free: 434.2 MiB)\n",
      "25/07/31 11:46:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 11:46:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/31 11:46:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on d7020f36ea8c:45503 in memory (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.21.0.4, executor 0, partition 0, PROCESS_LOCAL, 9764 bytes) \n",
      "25/07/31 11:46:41 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.21.0.4, executor 0, partition 1, PROCESS_LOCAL, 9837 bytes) \n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.21.0.4:37847 in memory (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.21.0.4:37847 (size: 122.8 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1898 ms on 172.21.0.4 (executor 0) (1/2)\n",
      "25/07/31 11:46:43 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 1896 ms on 172.21.0.4 (executor 0) (2/2)\n",
      "25/07/31 11:46:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/31 11:46:43 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 1.926 s\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 11:46:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:0, took 1.928338 s\n",
      "25/07/31 11:46:43 INFO DeltaFileFormatWriter: Start to commit write Job 449e8c7a-6754-404c-afad-d896b2583bcb.\n",
      "25/07/31 11:46:43 INFO DeltaFileFormatWriter: Write Job 449e8c7a-6754-404c-afad-d896b2583bcb committed. Elapsed time: 0 ms.\n",
      "25/07/31 11:46:43 INFO DeltaFileFormatWriter: Finished processing stats for write job 449e8c7a-6754-404c-afad-d896b2583bcb.\n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on d7020f36ea8c:45503 in memory (size: 122.8 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.21.0.4:37847 in memory (size: 122.8 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:43 INFO OptimisticTransaction: [tableId=5eafe551,txnId=6d9b8690] Attempting to commit version 1 with 3 actions with Serializable isolation level\n",
      "25/07/31 11:46:43 INFO DeltaLog: Creating a new snapshot v1 for commit version 1\n",
      "25/07/31 11:46:43 INFO DeltaLog: Loading version 1.\n",
      "25/07/31 11:46:43 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 2, totalFileSize: 2253)\n",
      "25/07/31 11:46:43 INFO DataSourceStrategy: Pruning directories with: \n",
      "25/07/31 11:46:43 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/31 11:46:43 INFO FileSourceStrategy: Post-Scan Filters: ((isnotnull(protocol#163.minReaderVersion) OR isnotnull(metaData#162.id)) OR (isnotnull(commitInfo#164.inCommitTimestamp) AND (version#165L = 1)))\n",
      "25/07/31 11:46:43 INFO CodeGenerator: Code generated in 39.049189 ms\n",
      "25/07/31 11:46:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 205.2 KiB, free 434.2 MiB)\n",
      "25/07/31 11:46:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)\n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on d7020f36ea8c:45503 (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:43 INFO SparkContext: Created broadcast 5 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195430 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/31 11:46:43 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Got job 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
      "25/07/31 11:46:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 55.3 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 434.1 MiB)\n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on d7020f36ea8c:45503 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/31 11:46:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "25/07/31 11:46:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.21.0.4, executor 0, partition 0, PROCESS_LOCAL, 10113 bytes) \n",
      "25/07/31 11:46:43 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (172.21.0.4, executor 0, partition 1, PROCESS_LOCAL, 10113 bytes) \n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.21.0.4:37847 (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 11:46:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.21.0.4:37847 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 11:46:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 89 ms on 172.21.0.4 (executor 0) (1/2)\n",
      "25/07/31 11:46:43 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 90 ms on 172.21.0.4 (executor 0) (2/2)\n",
      "25/07/31 11:46:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/07/31 11:46:43 INFO DAGScheduler: ResultStage 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.098 s\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 11:46:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/07/31 11:46:43 INFO DAGScheduler: Job 3 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.100990 s\n",
      "25/07/31 11:46:43 INFO Snapshot: [tableId=5eafe551-376f-4460-b535-eab981c95ca1] Created snapshot Snapshot(path=s3a://bronze/employe/_delta_log, version=1, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,1,ArrayBuffer(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398006; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753962403000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=e6e620de7e0a148edef79b0699261c7e versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962403000), checksumOpt=None)\n",
      "25/07/31 11:46:43 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://bronze/employe/_delta_log, version=1, metadata=Metadata(5eafe551-376f-4460-b535-eab981c95ca1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753962397724)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,1,ArrayBuffer(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753962398006; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1d4e421b2c8df791aa9f4c25438c39bc versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753962403000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=e6e620de7e0a148edef79b0699261c7e versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@6d0c6d66,1753962403000), checksumOpt=None)\n",
      "25/07/31 11:46:43 INFO OptimisticTransaction: [tableId=5eafe551,txnId=6d9b8690] Committed delta #1 to s3a://bronze/employe/_delta_log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o caminho do MinIO para armazenar os dados no formato Delta\n",
    "emp_path = \"s3a://bronze/employe\"\n",
    "\n",
    "# Criando a tabela de destino 'employe' no formato Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employe (\n",
    "    emp_id INT,\n",
    "    emp_name STRING,\n",
    "    dept_code STRING,\n",
    "    salary DOUBLE\n",
    ") USING DELTA LOCATION '{emp_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Inserindo valores fictícios na tabela employe\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe (emp_id, emp_name, dept_code, salary)\n",
    "VALUES \n",
    "    (1001, 'Alice', 'D101', 55000),\n",
    "    (1002, 'Bob', 'D102', 60000),\n",
    "    (1003, 'Charlie', 'D103', 75000),\n",
    "    (1004, 'David', 'D104', 65000),\n",
    "    (1005, 'Eve', 'D105', 70000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5f2976-7182-44d7-a309-e375dc7bf7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 11:47:04 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/07/31 11:47:04 INFO SparkUI: Stopped Spark web UI at http://d7020f36ea8c:4040\n",
      "25/07/31 11:47:04 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "25/07/31 11:47:04 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "25/07/31 11:47:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/07/31 11:47:04 INFO MemoryStore: MemoryStore cleared\n",
      "25/07/31 11:47:04 INFO BlockManager: BlockManager stopped\n",
      "25/07/31 11:47:04 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/07/31 11:47:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/07/31 11:47:04 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d935781-3fdb-44ab-b936-7b26674815a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

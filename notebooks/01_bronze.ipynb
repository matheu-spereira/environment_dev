{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409d3d-41a6-47ec-9474-b392da225d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from lib.spark_session import get_spark_session\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4902894e-cdeb-4eed-88f4-a0f81e6c440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: 8\n",
      "25/07/31 12:01:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/31 12:01:18 WARN DependencyUtils: Local jar /opt/spark/jars/openlineage-spark_2.12-1.33.0.jar does not exist, skipping.\n",
      "25/07/31 12:01:18 INFO SparkContext: Running Spark version 3.5.6\n",
      "25/07/31 12:01:18 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64\n",
      "25/07/31 12:01:18 INFO SparkContext: Java version 17.0.15\n",
      "25/07/31 12:01:18 INFO ResourceUtils: ==============================================================\n",
      "25/07/31 12:01:18 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/31 12:01:18 INFO ResourceUtils: ==============================================================\n",
      "25/07/31 12:01:18 INFO SparkContext: Submitted application: Bronze\n",
      "25/07/31 12:01:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/31 12:01:18 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "25/07/31 12:01:18 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/31 12:01:18 INFO SecurityManager: Changing view acls to: root\n",
      "25/07/31 12:01:18 INFO SecurityManager: Changing modify acls to: root\n",
      "25/07/31 12:01:18 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/31 12:01:18 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/31 12:01:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/07/31 12:01:19 INFO Utils: Successfully started service 'sparkDriver' on port 41305.\n",
      "25/07/31 12:01:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/31 12:01:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/31 12:01:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/31 12:01:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/31 12:01:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/31 12:01:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-947b6615-099e-4504-92fe-437ebb9cb19a\n",
      "25/07/31 12:01:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/07/31 12:01:19 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/31 12:01:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/31 12:01:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/07/31 12:01:19 INFO SparkContext: Added JAR file:///opt/spark/jars/postgresql-42.6.0.jar at spark://b5350baaaae2:41305/jars/postgresql-42.6.0.jar with timestamp 1753963278895\n",
      "25/07/31 12:01:19 INFO SparkContext: Added JAR file:///opt/spark/jars/hadoop-aws-3.3.4.jar at spark://b5350baaaae2:41305/jars/hadoop-aws-3.3.4.jar with timestamp 1753963278895\n",
      "25/07/31 12:01:19 INFO SparkContext: Added JAR file:///opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar at spark://b5350baaaae2:41305/jars/aws-java-sdk-bundle-1.12.262.jar with timestamp 1753963278895\n",
      "25/07/31 12:01:19 INFO SparkContext: Added JAR file:///opt/spark/jars/delta-spark_2.12-3.2.0.jar at spark://b5350baaaae2:41305/jars/delta-spark_2.12-3.2.0.jar with timestamp 1753963278895\n",
      "25/07/31 12:01:19 INFO SparkContext: Added JAR file:///opt/spark/jars/delta-storage-3.2.0.jar at spark://b5350baaaae2:41305/jars/delta-storage-3.2.0.jar with timestamp 1753963278895\n",
      "25/07/31 12:01:19 ERROR SparkContext: Failed to add file:/opt/spark/jars/openlineage-spark_2.12-1.33.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/jars/openlineage-spark_2.12-1.33.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/31 12:01:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "25/07/31 12:01:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.7:7077 after 17 ms (0 ms spent in bootstraps)\n",
      "25/07/31 12:01:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250731120119-0001\n",
      "25/07/31 12:01:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46367.\n",
      "25/07/31 12:01:19 INFO NettyBlockTransferService: Server created on b5350baaaae2:46367\n",
      "25/07/31 12:01:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/31 12:01:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b5350baaaae2, 46367, None)\n",
      "25/07/31 12:01:19 INFO BlockManagerMasterEndpoint: Registering block manager b5350baaaae2:46367 with 434.4 MiB RAM, BlockManagerId(driver, b5350baaaae2, 46367, None)\n",
      "25/07/31 12:01:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b5350baaaae2, 46367, None)\n",
      "25/07/31 12:01:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b5350baaaae2, 46367, None)\n",
      "25/07/31 12:01:19 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/events/app-20250731120119-0001.inprogress\n",
      "25/07/31 12:01:19 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "25/07/31 12:01:19 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "25/07/31 12:01:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250731120119-0001/0 on worker-20250731115710-172.18.0.6-43601 (172.18.0.6:43601) with 2 core(s)\n",
      "25/07/31 12:01:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20250731120119-0001/0 on hostPort 172.18.0.6:43601 with 2 core(s), 1024.0 MiB RAM\n",
      "25/07/31 12:01:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250731120119-0001/0 is now RUNNING\n",
      "25/07/31 12:01:19 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Generate SparkSession\n",
    "spark: SparkSession = get_spark_session(\"Bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5901d52-b711-4c91-9713-afce7f56f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 12:01:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/31 12:01:19 INFO SharedState: Warehouse path is 'file:/home/jupyter/notebooks/spark-warehouse'.\n",
      "25/07/31 12:01:21 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:44224) with ID 0,  ResourceProfileId 0\n",
      "25/07/31 12:01:21 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "25/07/31 12:01:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:45983 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.6, 45983, None)\n",
      "25/07/31 12:01:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/07/31 12:01:21 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/07/31 12:01:21 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
      "25/07/31 12:01:22 INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore)` is used for scheme `s3a`\n",
      "25/07/31 12:01:22 INFO DeltaLog: Loading version 2.\n",
      "25/07/31 12:01:22 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 3, totalFileSize: 3554)\n",
      "25/07/31 12:01:23 INFO DataSourceStrategy: Pruning directories with: \n",
      "25/07/31 12:01:23 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/31 12:01:23 INFO FileSourceStrategy: Post-Scan Filters: ((isnotnull(protocol#15.minReaderVersion) OR isnotnull(metaData#14.id)) OR (isnotnull(commitInfo#16.inCommitTimestamp) AND (version#17L = 2)))\n",
      "25/07/31 12:01:23 INFO CodeGenerator: Code generated in 252.156703 ms\n",
      "25/07/31 12:01:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 205.0 KiB, free 434.2 MiB)\n",
      "25/07/31 12:01:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)\n",
      "25/07/31 12:01:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b5350baaaae2:46367 (size: 36.0 KiB, free: 434.4 MiB)\n",
      "25/07/31 12:01:23 INFO SparkContext: Created broadcast 0 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 12:01:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6293233 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/31 12:01:23 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Got job 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
      "25/07/31 12:01:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 55.3 KiB, free 434.1 MiB)\n",
      "25/07/31 12:01:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 434.1 MiB)\n",
      "25/07/31 12:01:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b5350baaaae2:46367 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 12:01:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/31 12:01:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "25/07/31 12:01:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 10252 bytes) \n",
      "25/07/31 12:01:23 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 10113 bytes) \n",
      "25/07/31 12:01:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:45983 (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 12:01:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:45983 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:25 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1598 ms on 172.18.0.6 (executor 0) (1/2)\n",
      "25/07/31 12:01:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1608 ms on 172.18.0.6 (executor 0) (2/2)\n",
      "25/07/31 12:01:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/31 12:01:25 INFO DAGScheduler: ResultStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.643 s\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 12:01:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Job 0 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.670815 s\n",
      "25/07/31 12:01:25 INFO CodeGenerator: Code generated in 31.341728 ms            \n",
      "25/07/31 12:01:25 INFO Snapshot: [tableId=b893de42-8a34-424b-b221-f7809f421a38] Created snapshot Snapshot(path=s3a://bronze/employe/_delta_log, version=2, metadata=Metadata(13999822-0c95-41cf-9adb-dd3e28c9c648,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753963066507)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,2,WrappedArray(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753963066812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d295840fc60c655c69bc0b0245bde457 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963071812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ad8c36c5b7fdee11698130ca711ae789 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000002.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963077116; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b35987fd5b941067e6729f5adf48043d versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@46ff4775,1753963077116), checksumOpt=None)\n",
      "25/07/31 12:01:25 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://bronze/employe/_delta_log, version=2, metadata=Metadata(13999822-0c95-41cf-9adb-dd3e28c9c648,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753963066507)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,2,WrappedArray(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753963066812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d295840fc60c655c69bc0b0245bde457 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963071812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ad8c36c5b7fdee11698130ca711ae789 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000002.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963077116; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b35987fd5b941067e6729f5adf48043d versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@46ff4775,1753963077116), checksumOpt=None)\n",
      "25/07/31 12:01:25 INFO CreateDeltaTableCommand: Table is path-based table: false. Update catalog with mode: Create\n",
      "25/07/31 12:01:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/07/31 12:01:25 INFO CodeGenerator: Code generated in 5.704125 ms\n",
      "25/07/31 12:01:25 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Final stage: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0)\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/07/31 12:01:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 336.0 KiB, free 433.8 MiB)\n",
      "25/07/31 12:01:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 122.7 KiB, free 433.6 MiB)\n",
      "25/07/31 12:01:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b5350baaaae2:46367 (size: 122.7 KiB, free: 434.2 MiB)\n",
      "25/07/31 12:01:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 12:01:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/31 12:01:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "25/07/31 12:01:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9764 bytes) \n",
      "25/07/31 12:01:25 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 9837 bytes) \n",
      "25/07/31 12:01:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:45983 (size: 122.7 KiB, free: 434.2 MiB)\n",
      "25/07/31 12:01:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1609 ms on 172.18.0.6 (executor 0) (1/2)\n",
      "25/07/31 12:01:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1611 ms on 172.18.0.6 (executor 0) (2/2)\n",
      "25/07/31 12:01:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/07/31 12:01:27 INFO DAGScheduler: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 1.635 s\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 12:01:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 1.639095 s\n",
      "25/07/31 12:01:27 INFO DeltaFileFormatWriter: Start to commit write Job 149a5035-e99d-4269-84a5-0159018bf0bb.\n",
      "25/07/31 12:01:27 INFO DeltaFileFormatWriter: Write Job 149a5035-e99d-4269-84a5-0159018bf0bb committed. Elapsed time: 0 ms.\n",
      "25/07/31 12:01:27 INFO DeltaFileFormatWriter: Finished processing stats for write job 149a5035-e99d-4269-84a5-0159018bf0bb.\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b5350baaaae2:46367 in memory (size: 122.7 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.6:45983 in memory (size: 122.7 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b5350baaaae2:46367 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.6:45983 in memory (size: 17.2 KiB, free: 434.4 MiB)\n",
      "25/07/31 12:01:27 INFO OptimisticTransaction: [tableId=13999822,txnId=9871a1ca] Attempting to commit version 3 with 3 actions with Serializable isolation level\n",
      "25/07/31 12:01:27 INFO DeltaLog: Creating a new snapshot v3 for commit version 3\n",
      "25/07/31 12:01:27 INFO DeltaLog: Loading version 3.\n",
      "25/07/31 12:01:27 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 4, totalFileSize: 4855)\n",
      "25/07/31 12:01:27 INFO DataSourceStrategy: Pruning directories with: \n",
      "25/07/31 12:01:27 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/31 12:01:27 INFO FileSourceStrategy: Post-Scan Filters: ((isnotnull(protocol#133.minReaderVersion) OR isnotnull(metaData#132.id)) OR (isnotnull(commitInfo#134.inCommitTimestamp) AND (version#135L = 3)))\n",
      "25/07/31 12:01:27 INFO CodeGenerator: Code generated in 26.61826 ms\n",
      "25/07/31 12:01:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 205.2 KiB, free 434.0 MiB)\n",
      "25/07/31 12:01:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b5350baaaae2:46367 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:27 INFO SparkContext: Created broadcast 3 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 12:01:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8391035 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/31 12:01:27 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Got job 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
      "25/07/31 12:01:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 55.3 KiB, free 433.9 MiB)\n",
      "25/07/31 12:01:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 433.9 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b5350baaaae2:46367 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611\n",
      "25/07/31 12:01:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/31 12:01:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "25/07/31 12:01:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 10252 bytes) \n",
      "25/07/31 12:01:27 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 10252 bytes) \n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.6:45983 (size: 17.2 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:45983 (size: 36.0 KiB, free: 434.3 MiB)\n",
      "25/07/31 12:01:28 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 105 ms on 172.18.0.6 (executor 0) (1/2)\n",
      "25/07/31 12:01:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 108 ms on 172.18.0.6 (executor 0) (2/2)\n",
      "25/07/31 12:01:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/31 12:01:28 INFO DAGScheduler: ResultStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.114 s\n",
      "25/07/31 12:01:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/31 12:01:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/31 12:01:28 INFO DAGScheduler: Job 2 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.116474 s\n",
      "25/07/31 12:01:28 INFO Snapshot: [tableId=13999822-0c95-41cf-9adb-dd3e28c9c648] Created snapshot Snapshot(path=s3a://bronze/employe/_delta_log, version=3, metadata=Metadata(13999822-0c95-41cf-9adb-dd3e28c9c648,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753963066507)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,3,ArrayBuffer(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753963066812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d295840fc60c655c69bc0b0245bde457 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963071812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ad8c36c5b7fdee11698130ca711ae789 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000002.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963077116; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b35987fd5b941067e6729f5adf48043d versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000003.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963287000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=c9257b9ea2fbe830abf4c9a9962fcaef versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@46ff4775,1753963287000), checksumOpt=None)\n",
      "25/07/31 12:01:28 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://bronze/employe/_delta_log, version=3, metadata=Metadata(13999822-0c95-41cf-9adb-dd3e28c9c648,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"emp_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"emp_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dept_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1753963066507)), logSegment=LogSegment(s3a://bronze/employe/_delta_log,3,ArrayBuffer(S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000000.json; isDirectory=false; length=952; replication=1; blocksize=33554432; modification_time=1753963066812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d295840fc60c655c69bc0b0245bde457 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000001.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963071812; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ad8c36c5b7fdee11698130ca711ae789 versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000002.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963077116; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b35987fd5b941067e6729f5adf48043d versionId=null, S3AFileStatus{path=s3a://bronze/employe/_delta_log/00000000000000000003.json; isDirectory=false; length=1301; replication=1; blocksize=33554432; modification_time=1753963287000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=c9257b9ea2fbe830abf4c9a9962fcaef versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@46ff4775,1753963287000), checksumOpt=None)\n",
      "25/07/31 12:01:28 INFO OptimisticTransaction: [tableId=13999822,txnId=9871a1ca] Committed delta #3 to s3a://bronze/employe/_delta_log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o caminho do MinIO para armazenar os dados no formato Delta\n",
    "emp_path = \"s3a://bronze/employe\"\n",
    "\n",
    "# Criando a tabela de destino 'employe' no formato Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employe (\n",
    "    emp_id INT,\n",
    "    emp_name STRING,\n",
    "    dept_code STRING,\n",
    "    salary DOUBLE\n",
    ") USING DELTA LOCATION '{emp_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Inserindo valores fictícios na tabela employe\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe (emp_id, emp_name, dept_code, salary)\n",
    "VALUES \n",
    "    (1001, 'Alice', 'D101', 55000),\n",
    "    (1002, 'Bob', 'D102', 60000),\n",
    "    (1003, 'Charlie', 'D103', 75000),\n",
    "    (1004, 'David', 'D104', 65000),\n",
    "    (1005, 'Eve', 'D105', 70000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5f2976-7182-44d7-a309-e375dc7bf7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 12:01:28 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/07/31 12:01:28 INFO SparkUI: Stopped Spark web UI at http://b5350baaaae2:4040\n",
      "25/07/31 12:01:28 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "25/07/31 12:01:28 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "25/07/31 12:01:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/07/31 12:01:28 INFO MemoryStore: MemoryStore cleared\n",
      "25/07/31 12:01:28 INFO BlockManager: BlockManager stopped\n",
      "25/07/31 12:01:28 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/07/31 12:01:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/07/31 12:01:28 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d935781-3fdb-44ab-b936-7b26674815a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
